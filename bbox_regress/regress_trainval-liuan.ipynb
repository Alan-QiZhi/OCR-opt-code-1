{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import xml.dom.minidom\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取txt文件\n",
    "train_txt = open('/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/txtsave/train.txt')   \n",
    "val_txt = open('/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/txtsave/val.txt')\n",
    "train_content = train_txt.readlines()   #保存的train.txt中的内容\n",
    "val_content = val_txt.readlines()  #保存的val.txt中的内容\n",
    "# for linetr in train_content:\n",
    "#     print (\"train_content\",linetr.rstrip('\\n'))\n",
    "# for lineva in val_content:\n",
    "#     print (\"val_content\",lineva.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据txt文件读取图像数据,并归一化图像，并保存缩放比例\n",
    "train_imgs=[]#缩放后的图像尺寸\n",
    "train_imgs_ratio=[] #width 缩放比，height缩放比\n",
    "val_imgs=[]\n",
    "val_imgs_ratio=[]\n",
    "\n",
    "\n",
    "h=48\n",
    "w=192  #归一化的尺寸\n",
    "c=3   #通道\n",
    "\n",
    "\n",
    "for linetr in train_content:\n",
    "    img_path='/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/cut_any_result_zc_fz/'+linetr.rstrip('\\n')+'.jpg'\n",
    "    img = cv2.imread(img_path)  #读取原图\n",
    "#     print(\"image_name\", str(linetr.rstrip('\\n')))\n",
    "#     print(\"imgshape\", img.shape)\n",
    "    imgresize= cv2.resize(img,(w,h)) #图像归一化\n",
    "    ratio = np.array([imgresize.shape[0]/img.shape[0], imgresize.shape[1]/img.shape[1]],np.float32) #height缩放比 ,width 缩放比，\n",
    "    train_imgs_ratio.append(ratio)    \n",
    "    train_imgs.append(imgresize)\n",
    "train_img_arr = np.asarray(train_imgs,np.float32)  #保存训练图像数据的列表  h w c\n",
    "print(len(train_img_arr),len(train_imgs_ratio))\n",
    "\n",
    "for  lineva in val_content:\n",
    "    img_path='/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/cut_any_result_zc_fz/'+lineva.rstrip('\\n')+'.jpg'\n",
    "    img = cv2.imread(img_path) # h w c\n",
    "    imgresize= cv2.resize(img,(w,h))  #h w c\n",
    "    ratio = np.array([imgresize.shape[0]/img.shape[0], imgresize.shape[1]/img.shape[1]],np.float32) #height缩放比, width 缩放比，\n",
    "    val_imgs_ratio.append(ratio)\n",
    "    val_imgs.append(imgresize)\n",
    "   # print(imgresize.shape[0], imgresize.shape[1], imgresize.shape[2])\n",
    "val_img_arr = np.asarray(val_imgs,np.float32)  #保存验证图像的数据的列表 h w c\n",
    "\n",
    "# print(len(val_img_arr),len(val_imgs_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据txt文件读取xml,并获取xml中的坐标（xmin,ymin,xmax,ymax）(x表示width,y表示height),并获取经过缩放后的坐标\n",
    "train_xml=[]  #保存标记的边框坐标\n",
    "train_xml_resize=[]  #保存标记的边框坐标经过缩放后的坐标，缩放比与图像归一化的缩放比\n",
    "val_xml=[]\n",
    "val_xml_resize=[]\n",
    "for linetr in train_content:\n",
    "    xml_path='/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/xml_zc_fz_6000_2nd/'+linetr.rstrip('\\n')+'.xml'\n",
    "    print(xml_path)\n",
    "    xml_DomTree = xml.dom.minidom.parse(xml_path)\n",
    "    xml_annotation = xml_DomTree.documentElement\n",
    "    xml_object = xml_annotation.getElementsByTagName('object')\n",
    "    xml_bndbox = xml_object[0].getElementsByTagName('bndbox')\n",
    "    xmin_list = xml_bndbox[0].getElementsByTagName('xmin')\n",
    "    xmin = int(xmin_list[0].childNodes[0].data)\n",
    "    ymin_list = xml_bndbox[0].getElementsByTagName('ymin')\n",
    "    ymin = int(ymin_list[0].childNodes[0].data)\n",
    "    xmax_list = xml_bndbox[0].getElementsByTagName('xmax')\n",
    "    xmax = int(xmax_list[0].childNodes[0].data)\n",
    "    ymax_list = xml_bndbox[0].getElementsByTagName('ymax')\n",
    "    ymax = int(ymax_list[0].childNodes[0].data)\n",
    "    coordinate = np.array([ymin, xmin ,ymax, xmax],np.int)  #h w h w\n",
    "    train_xml.append(coordinate) #保存训练图像的xml的坐标\n",
    "#     print(\"bbox:\", coordinate)\n",
    "# print(len(train_xml))\n",
    "\n",
    "for lineva in val_content:\n",
    "    xml_path='/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/dataset/xml_zc_fz_6000_2nd/'+lineva.rstrip('\\n')+'.xml'\n",
    "    print(xml_path)\n",
    "    xml_DomTree = xml.dom.minidom.parse(xml_path)\n",
    "    xml_annotation = xml_DomTree.documentElement\n",
    "    xml_object = xml_annotation.getElementsByTagName('object')\n",
    "    xml_bndbox = xml_object[0].getElementsByTagName('bndbox')\n",
    "    xmin_list = xml_bndbox[0].getElementsByTagName('xmin')\n",
    "    xmin = int(xmin_list[0].childNodes[0].data)\n",
    "    ymin_list = xml_bndbox[0].getElementsByTagName('ymin')\n",
    "    ymin = int(ymin_list[0].childNodes[0].data)\n",
    "    xmax_list = xml_bndbox[0].getElementsByTagName('xmax')\n",
    "    xmax = int(xmax_list[0].childNodes[0].data)\n",
    "    ymax_list = xml_bndbox[0].getElementsByTagName('ymax')\n",
    "    ymax = int(ymax_list[0].childNodes[0].data)\n",
    "    coordinate = np.array([ymin, xmin ,ymax, xmax],np.int)\n",
    "    val_xml.append(coordinate)  #保存验证图像的xml的坐标\n",
    "# print(len(val_xml))   \n",
    "\n",
    "for i in range(0,len(train_imgs_ratio)):\n",
    "    ymin_ratio=train_xml[i][0]*train_imgs_ratio[i][0]\n",
    "    xmin_ratio=train_xml[i][1]*train_imgs_ratio[i][1]\n",
    "    ymax_ratio=train_xml[i][2]*train_imgs_ratio[i][0]\n",
    "    xmax_ratio=train_xml[i][3]*train_imgs_ratio[i][1]\n",
    "    coordinate_ratio = np.array([ymin_ratio, xmin_ratio ,ymax_ratio, xmax_ratio],np.float32)\n",
    "    train_xml_resize.append(coordinate_ratio)  #保存训练图像的标记的xml的缩放后的坐标\n",
    "    \n",
    "for i in range(0,len(val_imgs_ratio)):\n",
    "    ymin_ratio=val_xml[i][0]*val_imgs_ratio[i][0]\n",
    "    xmin_ratio=val_xml[i][1]*val_imgs_ratio[i][1]\n",
    "    ymax_ratio=val_xml[i][2]*val_imgs_ratio[i][0]\n",
    "    xmax_ratio=val_xml[i][3]*val_imgs_ratio[i][1]\n",
    "    coordinate_ratio = np.array([ymin_ratio, xmin_ratio ,ymax_ratio, xmax_ratio],np.float32)\n",
    "    val_xml_resize.append(coordinate_ratio)   #保存训练验证图像的标记的xml的缩放后的坐标\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按批次取数据，获取batchsize数据\n",
    "# inputs 图像数据  归一化后的数据\n",
    "# targets xml坐标数据  归一化后的数据\n",
    "def getbatches(inputs=None, targets=None, batch_size=None, shuffle=False):    \n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]#其实就是按照batchsize做切片\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]#这个yield每次都是遇到了就返回类似于关键字return\n",
    "        #但是下次执行的时候就是从yield后面的代码进行继续，此时这个函数不是普通函数而是一个生成器了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数smoothL1范数\n",
    "def abs_smooth(x):\n",
    "    \"\"\"Smoothed absolute function. Useful to compute an L1 smooth error.\n",
    "\n",
    "    Define as:\n",
    "        x^2 / 2         if abs(x) < 1\n",
    "        abs(x) - 0.5    if abs(x) > 1\n",
    "    We use here a differentiable definition using min(x) and abs(x). Clearly\n",
    "    not optimal, but good enough for our purpose!\n",
    "    \"\"\"\n",
    "    absx = tf.abs(x)\n",
    "    minx = tf.minimum(absx, 1)\n",
    "    r = 0.5 * ((absx - 1) * minx + absx)#这个地方打开会有平方项\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#构建网络结构\n",
    "\n",
    "input_data = tf.placeholder(tf.float32,shape=[None,h,w,c],name='x')  #输入的图像数据（归一化后的图像数据）\n",
    "input_bound = tf.placeholder(tf.float32,shape=[None,None],name='y') #输入的标记的边框坐标数据（缩放后的xml坐标）\n",
    "prob=tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "#第一个卷积层（192——>96) （48--》24）\n",
    "#conv1 = slim.repeat(input_data, 2, slim.conv2d, 32, [3, 3], scope='conv1')\n",
    "conv1 = slim.conv2d(input_data,  32, [3, 3], scope='conv1')##32是指卷积核的个数，[3, 3]是指卷积核尺寸，默认步长是[1,1]\n",
    "pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')#[2,2]是池化步长\n",
    "\n",
    "#第二个卷积层（96-48） （24-》12）\n",
    "#conv2 =  slim.repeat(pool1, 2, slim.conv2d, 64, [3, 3], scope='conv2')\n",
    "conv2 = slim.conv2d(pool1, 64, [3, 3], scope='conv2')\n",
    "pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n",
    "\n",
    "#第三个卷积层（48-24） （12-》6）\n",
    "#conv3 = slim.repeat(pool2, 2, slim.conv2d, 128, [3, 3], scope='conv3')\n",
    "conv3 = slim.conv2d(pool2, 128, [3, 3], scope='conv3')\n",
    "pool3 = slim.max_pool2d(conv3, [2, 2], scope='pool3')\n",
    "\n",
    "#第四个卷积层（24） （6）\n",
    "conv4 = slim.conv2d(pool3, 256 ,[3, 3], scope='conv4')\n",
    "dropout = tf.layers.dropout(conv4, rate=prob, training=True)\n",
    "#dropout = tf.nn.dropout(conv4,keep_prob) \n",
    "#pool4 = slim.max_pool2d(conv4, [2, 2], scope='pool4')\n",
    "\n",
    "#第五个卷积层（24-12） （6-》3）\n",
    "#conv5 = slim.repeat(dropout, 2, slim.conv2d, 128, [3, 3], scope='conv5')\n",
    "conv5 = slim.conv2d(dropout , 128, [3, 3], scope='conv5')\n",
    "pool5 = slim.max_pool2d(conv5, [2, 2], scope='pool5')\n",
    "\n",
    "#第六个卷积层（12-6） （3-》1）\n",
    "#conv6 = slim.repeat(pool5, 2, slim.conv2d, 64, [3, 3], scope='conv6')\n",
    "conv6 = slim.conv2d(pool5, 64, [3, 3], scope='conv6')\n",
    "pool6 = slim.max_pool2d(conv6, [2, 2], scope='pool6')\n",
    "\n",
    "reshape = tf.reshape(pool6, [-1, 6 * 1 * 64])\n",
    "# print(reshape.get_shape())\n",
    "\n",
    "fc = slim.fully_connected(reshape, 4, scope='fc')  \n",
    "# print(fc)\n",
    "# print(input_data)\n",
    "\n",
    "'''\n",
    "#第七个卷积层（6-3） （1-》1）\n",
    "conv7 = slim.conv2d(pool6,  32, [3, 3], scope='conv7')\n",
    "pool7 = slim.max_pool2d(conv7, [2, 2], scope='pool7')\n",
    "\n",
    "conv8 = slim.conv2d(pool7, 4, [3, 3], padding=None, activation_fn=None,scope='conv8')\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epoch =16000\n",
    "batch_size= 8\n",
    "print (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.expand_dims(1. * 1., axis=-1)\n",
    "loss = abs_smooth(fc - input_bound)#fc层和输入标签的差，用平滑L2范数做损失函数\n",
    "# print(loss)\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)#优化用的adam，学习率0.001\n",
    "\n",
    "#correct_prediction = tf.equal(fc, input_bound)\n",
    "#correct_prediction = tf.equal(tf.cast(fc,tf.int32), tf.cast(input_bound, tf.int32))\n",
    "\n",
    "temp_acc = tf.abs(tf.cast(fc,tf.int32) - tf.cast(input_bound, tf.int32)) #fc出来之后的和标签做个差值\n",
    "compare_np = np.ones((batch_size,4), np.int32) #建立一个和batch_size一样大小，4通道的compare_np\n",
    "compare_np[:] = 3\n",
    "print(compare_np)\n",
    "compare_tf = tf.convert_to_tensor(compare_np) #\n",
    "# print(compare_tf)\n",
    "correct_prediction = tf.less(temp_acc,compare_tf)  ##temp_acc对应的元素如果比compare_tf对应的小，那么对应位置返回true\n",
    "# print(correct_prediction)\n",
    "loss = tf.div(tf.reduce_sum(loss * weights), batch_size, name='value')##求张量沿着某个方向的和，求完后可以降维度\n",
    "tf.summary.scalar('loss',loss) #可视化观看常量  \n",
    "# print(loss)\n",
    "accuracy= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))###tf.cast函数转换类型###\n",
    "#tf.summary.scalar('accuracy',accuracy) #可视化观看常量  \n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prob)\n",
    "\n",
    "# pb_file_path = '/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000/bbox_pb_model/ocr_bboxregress_batch16_epoch10000.pb'\n",
    "pb_file_path = '/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/bbox_pb_model_2nd/ocr_bbox_batch16_epoch'\n",
    "\n",
    "\n",
    "# 设置可见GPU\n",
    "gpu_no = '3' # or '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_no\n",
    "#定义TensorFlow配置\n",
    "config = tf.ConfigProto()\n",
    "#配置GPU内存分配方式\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "\n",
    "\n",
    "sess=tf.InteractiveSession(config = config)  \n",
    "\n",
    "#////////////////////////////////\n",
    "#ckpt = tf.train.get_checkpoint_state('/home/data/wangchongjin/ad_image/model_save/')\n",
    "#saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path +'.meta')   # 载入图结构，保存在.meta文件中\n",
    "#saver.restore(sess,ckpt.model_checkpoint_path)  \n",
    "#//////////////////////////////////\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"/data/liuan/jupyter/root/project/keras-retinanet-master/bbox_fz_zc_006000new/record_graph\", sess.graph_def)\n",
    "\n",
    "#saver = tf.train.Saver() # 声明tf.train.Saver类用于保存模型\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in getbatches(train_img_arr, train_xml_resize, batch_size, shuffle=False):\n",
    "        _,err,acc=sess.run([train_op,loss,accuracy], feed_dict={input_data: x_train_a, input_bound: y_train_a, prob: 0.5})\n",
    "        train_loss += err\n",
    "        train_acc += acc\n",
    "        n_batch += 1\n",
    "        \n",
    "#     print(epoch)\n",
    "#     print(\"   train loss: %f\" % (train_loss/ n_batch))\n",
    "#     print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
    "  \n",
    "\n",
    "\n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in getbatches(val_img_arr, val_xml_resize, batch_size, shuffle=False):\n",
    "        err,acc = sess.run([loss,accuracy], feed_dict={input_data: x_val_a, input_bound: y_val_a, prob: 0})\n",
    "        #print(err)\n",
    "        val_loss += err\n",
    "        val_acc += acc\n",
    "        n_batch += 1\n",
    "\n",
    "        rs =sess.run([merged], feed_dict={input_data: x_val_a, input_bound: y_val_a, prob: 0})\n",
    "        if n_batch is batch_size:\n",
    "            writer.add_summary(rs[0],epoch)\n",
    "   \n",
    "#     print(\"   validation loss: %f\" % (val_loss/ n_batch))\n",
    "#     print(\"   validation acc: %f\" % (val_acc/ n_batch))\n",
    "    \n",
    "    \n",
    "#    saver.save(sess, \"/home/data/wangchongjin/ad_image/model_save_new/ad.ckpt\")\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['fc/Relu'])\n",
    "\n",
    "    with tf.gfile.FastGFile(pb_file_path + '_' + str(epoch) + '.pb', mode='wb') as f:    \n",
    "        f.write(constant_graph.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
